# EU AI Code of Practice

Audio format: https://youtu.be/nJxiahNFKSQ

## The AI Code of Practice: A Responsible AI Guide
Okay, imagine the AI Code of Practice as a very important rulebook for companies that create super-smart AI models, especially the really powerful ones. 
Think of it like a guide for building new, amazing technology, but making sure it's built the right way.

The main goal of this whole rulebook is like trying to make sure that:
- AI is Good for Everyone  
  It wants to make sure that AI helps our society and economy work better, and that people can trust AI.
  It's about promoting "human-centric and trustworthy AI," which means AI that works for people and that we can rely on.
- People Stay Safe  
  A huge part of it is making sure that AI protects our health, safety, and basic rights (like your right to privacy or not to be unfairly treated).
  It wants to guard against any "harmful effects" that AI could have.
- New Ideas Can Still Grow
  Even while keeping everyone safe, the Code also wants to help new, cool, and safe AI technologies to develop and "support innovation".

Now, to make all that happen, the Code has a few specific jobs:
- It's a Helpful Guide for Companies  
  It's meant to be a document that helps companies show that they are following the rules set out in a bigger law called the "AI Act".
  It's like a detailed checklist or instruction manual for them.
- It Helps the AI Police (The AI Office) Check Up  
  It makes sure that the companies creating these powerful AI models actually follow their promises.
  It also helps a special group called the "AI Office" to check if these companies are doing a good job of keeping their AI safe and fair.

So, in short, this Code of Practice is like a parent's guide for powerful AI. 
It tells AI companies how to raise their AI to be responsible, safe, and helpful, making sure it doesn't cause trouble, respects everyone's rights, and grows up to be a force for good, 
all while the "AI Office" is watching to make sure they're following the rules!

## AI Code: Transparency & Model Chapter

Okay, let's dive into the Transparency Chapter! Remember how we talked about the AI Code of Practice being like a special rulebook for grown-ups who build and use powerful AI models? 
[Our conversation history] Well, the Transparency Chapter is all about being open and clear about how these AI models work [My previous explanation, based on source objective].
Think of it like this: When you get a new, complicated LEGO set, it comes with an instruction manual, right? 
That manual tells you what pieces are inside, how to build it, and what the finished model is supposed to do. 
The Transparency Chapter is asking AI companies to make a similar "manual" for their AI models.

Here’s what the Transparency Chapter mainly focuses on:
- The "Manual" for AI Models (Model Documentation):
  - AI companies have to create and keep updated a special document called "Model Documentation". This is like the official instruction manual for their AI model.
  - They need to describe this policy in a single document.
  - They also need to assign people within their company to be responsible for making sure this "manual" is kept updated and followed.
  - This document isn't just made once; it needs to be updated whenever there are important changes to the AI model. They even have to keep old versions for 10 years.
- What's Inside the AI "Manual"? (Key Information): The "Model Documentation" needs to include lots of different information so that people can understand the AI model properly.
  Imagine this as different sections of your LEGO manual:
  - General Information  
    Like the model's name (e.g., "Llama 3.1-405B"), when it was first released.
    If the model was built by changing another AI model (like a "modification" or "fine-tuning"), it needs to list the names of those previous models.
  - Model Properties  
    This includes a general description of how the model is built (its "architecture"), what kind of information it can take in (like text, images, audio, video) and what it can produce.
    It also tells you the total size of the model (how many "parameters" it has, which is like how complex it is).
  - How to Use It  
    It needs to tell you the "Acceptable Use Policy".
    This policy explains what the AI is meant to be used for (like "productivity enhancement" or "translation") and what it is not allowed to be used for.
    It also lists the types of AI systems the model can, or shouldn't, be put into, and what hardware or software you need to use it.
  - How It Learned (Training Process)  
    A description of the main steps in how the AI model was "trained" or learned. This is like explaining how your LEGO set was designed and tested.
  - Information About the Data It Used  
    What types of data were used for training (text, images, audio, video).
    Where the data came from (like from the internet, private datasets, or user data). It should also mention how much data was used (like in "tokens" or "images").
    Important details about how the data was cleaned and processed, and any steps taken to detect unsuitable content (like illegal content) or biases in the data.
  - Energy and Computing Power Used  
    How much energy (in Megawatt-hours) and computing power was used to train the model, and how they measured or estimated this.
- Who Gets to See the "Manual"?
  - Downstream Providers  
    These are other companies that want to use the AI model in their own AI systems (like putting the AI "brain" into a new robot).
    They need to understand the model's capabilities and limitations to integrate it correctly and follow their own rules under the AI Act. So, the "manual" helps them.
  - The AI Office and National Authorities  
    These are like the government groups that check on AI.
    They can ask for specific parts of the "manual" when they need it, especially to check if a "high-risk" AI system (a very important one) that uses this model is following the rules.
  - The Public  
    AI companies are encouraged to make parts of this information public so that everyone can be more aware and understand how these powerful AIs work.
- Protecting Secrets
  - Even though there's a lot of sharing, the Code also says that confidential business information or "trade secrets" must be protected when this information is shared.
    It’s like keeping a secret recipe safe [My previous explanation].

In short, the Transparency Chapter is all about making sure that the people who use AI models, and the people who regulate them, 
have a clear and detailed understanding of what those models are, how they were made, and what they are designed to do. 
It’s like having a well-written, honest instruction manual for every complex AI "machine" out there.

## AI Code of Practice: Copyright Chapter Explained

Okay, let's talk about the Copyright Chapter of the AI Code of Practice!
Remember how we just chatted about the Transparency Chapter being like a detailed instruction manual for AI models? 
[My previous explanation] Well, the Copyright Chapter is a different part of that special rulebook, and it's all about making sure that AI companies respect people's creative work, like books, music, art, and videos.
Imagine you're building something awesome, like a new video game or a cool song. You wouldn't want someone else to just copy it and use it without your permission, right? 
That's what copyright is for – it protects creators' rights. This chapter makes sure AI companies play fair with those rights.

Here’s what the Copyright Chapter mostly focuses on:
- Having a "Fair Play" Copyright Policy:
  - AI companies have to create an official plan, called a "copyright policy", that explains how they will follow all the rules about copyright in the European Union.
  - This plan needs to be written down in one single document and kept up-to-date.
  - They also need to assign specific people within their company to be in charge of making sure this policy is followed.
  - Companies are even encouraged to share a summary of this policy publicly, so everyone knows how they handle copyrighted material.
- Rules for How AI "Learns" from the Internet (Web Crawling):
  - When AI models "learn" by looking at tons of information on the internet (which is called "web crawling" or "text and data mining"), they have to be very careful.
  - Only Use What's Lawful: They can only look at and use content that is legally accessible.
    This means they can't sneak around "paywalls" (like when you have to pay to read an article) or ignore technical rules that say "no entry".
  - Respect "No Entry" Signs (Robots.txt): Websites often have a special file called "robots.txt" which is like a "do not enter" sign for web crawlers.
    AI companies have to make sure their web crawlers read and follow these instructions.
  - Look for Other "Rights Reserved" Messages: They also need to be able to find and respect other ways creators might say, "Don't use my stuff for this purpose!"
    These messages can be hidden in the digital information of the content (like "metadata").
- Making Sure AI Doesn't Copy Badly (Copyright-Infringing Outputs):
  - The goal is to stop AI models from creating things that unfairly copy or infringe on copyrighted works [22, 23a].
  - AI companies need to put in place "technical safeguards" (which are like digital fences or filters) to prevent the AI from spitting out outputs that are too similar to copyrighted training content [23a].
  - They also have to include rules in their "acceptable use policy" (which is like a list of rules for people using their AI) that forbid users from using the AI to make copyrighted copies [23b].
    This applies whether the AI company uses the model themselves or gives it to another company.
- Easy Way for Creators to Complain (Point of Contact):
  - AI companies must have a clear and easy way for creators ("rightsholders") to get in touch with them if they think their copyrighted work has been used wrongly.
  - They also need to set up a system for handling complaints in a fair and timely manner.

In simple terms, this chapter is like saying: "AI models are powerful learners, but they must learn responsibly. 
They can't just take any content they find, and they have to be careful not to copy others' hard work. 
And if a creator feels their work has been wronged, there needs to be a clear way to address it." 
It’s like teaching a very smart student to be honest and respectful about where they get their information and how they use it, 
always giving credit where it's due, and never stealing someone else's homework!

## AI Safety and Security Code of Practice

Alright, let's talk about the Safety and Security Chapter of the AI Code of Practice! 
Imagine you're building a super-advanced robot, like one that could help people, but also has some very powerful tools. 
You wouldn't just let it loose without making sure it's safe and won't accidentally cause problems, right? 
You'd want it to be reliable and protected from people who might try to mess with it.
This chapter of the AI Code is all about making sure that powerful AI models are built in a super safe way and are protected from bad actors. 
It's especially for the really big, important AI models that could cause big problems if they go wrong. These big risks are called "systemic risks".

Here's how they plan to make AI models safe and secure:
- Having a Master Safety Plan (Safety and Security Framework):
  - AI companies have to create a "Safety and Security Framework," which is like a big master plan for how they will keep their AI models safe and make sure the risks are acceptable.
  - They need to make this plan, then actually do what the plan says, and keep updating it as the AI changes or new risks are found.
  - They even have to tell the "AI Office" (which is like the main group checking on AI) about their plan.
- Finding the Dangers (Systemic Risk Identification):
  - The first step is to find all the possible big dangers that the AI model could create.
  - This means thinking about risks to things like public health, general safety, public security, people's fundamental rights (like privacy), and even society as a whole.
  - They have to imagine different "risk scenarios" – like stories about how something bad could happen.
- Understanding the Dangers Better (Systemic Risk Analysis):
  - Once they've found the potential dangers, they need to really understand them.
  - This involves gathering information (like researching similar problems or looking at how other AI models behave).
  - They have to test their AI model in smart ways, like putting it through different challenges to see what it can do and if it shows any unexpected behaviors. This is called "model evaluation".
  - They also need to figure out how likely a bad thing is to happen and how bad it would be if it did (like "critical" or "moderate" risk).
  - And even after the AI model is out there, they have to keep watching it (this is called "post-market monitoring") to make sure no new problems pop up. This includes collecting feedback from users and checking for weird activity.
- Deciding if the Risk is Okay (Systemic Risk Acceptance Determination):
  - After all that checking, they need to decide if the risks from their AI model are "acceptable".
  - They set up special rules called "systemic risk acceptance criteria" to help them make this decision.
  - If the risks are not okay, or if they predict they might become not okay soon, the company must take steps to fix it before continuing to develop or release the AI.
    This could mean stopping the release, or adding more safety features.
- Making the AI Safe (Safety Mitigations):
  - These are the actual things they do to prevent harm. They need to put in "appropriate safety mitigations".
  - This can include things like cleaning the data the AI learns from to remove bad stuff, filtering the AI's outputs so it doesn't say or do harmful things, or even training the AI to refuse certain dangerous requests.
  - They might also slowly give access to the AI to make sure it's safe before wider release.
- Protecting the AI (Security Mitigations):
  - This part is about protecting the AI itself, its "brain" (which they call "model parameters"), and the computers it runs on from being hacked, stolen, or messed with.
  - They have to set a "Security Goal" that says who they're trying to protect against (like external hackers or even people inside the company).
  - Then they put in strong security measures like making sure only authorized people can access important parts of the AI, encrypting sensitive information, and doing background checks on employees.
- Telling Everyone What They Did (Safety and Security Model Reports):
  - AI companies have to write detailed reports called "Model Reports" for the AI Office.
  - These reports explain everything: what the model is, how it's supposed to be used, all the dangers they found, how they analyzed them, and all the safety and security steps they put in place.
  - They have to keep these reports up-to-date and send new versions to the AI Office regularly, especially if something big changes.
- Who's in Charge (Systemic Risk Responsibility Allocation):
  - It's important that everyone knows their job when it comes to AI safety.
  - Companies have to clearly say who is responsible for overseeing risks, managing them, supporting safety efforts, and checking that everything is being done properly.
  - They also need to make sure these people have enough resources (like staff, money, and computing power).
- Talking About Accidents (Serious Incident Reporting):
  - If something bad does happen with an AI model (a "serious incident"), the company has to have ways to track, document, and report it to the AI Office and other authorities without too much delay.
  - They have strict timelines for reporting, especially if it involves very serious things like critical infrastructure disruption, cybersecurity breaches, or harm to people. 
    They also have to explain what caused it and what they did to fix it.
- Keeping Good Records and Being Open (Additional Documentation and Transparency):
  - Companies need to keep detailed records of how they're following all these rules.
  - They are also encouraged to share summarized versions of their safety plans and reports with the public when it's necessary to understand or reduce risks, while still protecting important business secrets.

This whole chapter is like having a really good plan for a building project: you identify all the possible ways the building could fall down or hurt someone, 
you test your materials, you put in strong supports and safety features, you protect it from storms and bad people, 
you have clear roles for who is responsible for what, you keep detailed blueprints, and if anything goes wrong, you have a clear way to report and fix it. 
It's all about making sure these powerful AI models are built, used, and monitored with the highest level of care and responsibility.

## Learning Resource
- https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai
